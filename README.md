# ğŸ’ SmartPDF Chat

**SmartPDF Chat** is an AI-powered application built with **Streamlit**, **LangChain**, and **Ollama**, allowing users to **chat with their PDF documents** or have general **AI conversations** using locally hosted LLMs (like **Llama 3** or **DeepSeek R1**).  

It intelligently extracts text from PDFs, creates vector embeddings for semantic understanding, and answers user questions accurately based on the uploaded content.

---

## ğŸš€ Features

âœ… **Chat with PDFs** â€” Upload any PDF (NCERT, research papers, notes, etc.) and ask questions about the content.  
âœ… **General AI Chat** â€” Talk with powerful AI models like *Llama 3* or *DeepSeek R1* for general queries.  
âœ… **Offline/Local AI** â€” Uses **Ollama** to run LLMs locally (no OpenAI API key required).  
âœ… **Smart Embeddings** â€” Uses **HuggingFace Sentence Transformers** and **FAISS** for efficient semantic search.  
âœ… **Streamlit UI** â€” Clean and interactive interface for effortless use.  

---

## ğŸ§  Tech Stack

| Component | Technology |
|------------|-------------|
| **Frontend/UI** | Streamlit |
| **PDF Parsing** | PyPDF2 |
| **AI Models** | Ollama (Llama 3, DeepSeek R1) |
| **Text Splitting** | LangChain RecursiveCharacterTextSplitter |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) |
| **Vector Store** | FAISS |
| **Environment Variables** | python-dotenv 

---

## âš™ï¸ Installation Guide

Follow these steps to run SmartPDF Chat locally ğŸ‘‡  

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/SmartPDF-Chat.git
cd SmartPDF-Chat

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
venv\Scripts\activate        # On Windows
# OR
source venv/bin/activate     # On macOS/Linux

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Start Ollama Server
Make sure Ollama is installed and running locally.
To start Ollama, open a new terminal and run:
ollama serve

Then, pull the required models:
ollama pull llama3
ollama pull deepseek-r1:7b

5ï¸âƒ£ Run the Application
streamlit run app.py


ğŸ“„ Usage

1.Upload PDFs from the sidebar under ğŸ“„ PDF Processing.
2.Click Submit & Process PDFs to create vector embeddings.
3.Go to the ğŸ“„ Chat with PDF tab and start asking questions related to your uploaded documents.
4.Use the ğŸ¤– Chat with AI tab for general chat or coding-related questions.
5.You can switch between Llama 3 and DeepSeek R1 in the sidebar model selector.


ğŸ§‘â€ğŸ’» Example Questions
ğŸ“„ Chat with PDF:

â€œWhat are the main topics discussed in Chapter 5 of this biology textbook?â€
â€œSummarize the abstract of this research paper.â€
â€œWho are the authors mentioned in this paper?â€

ğŸ¤– Chat with AI:

â€œExplain overfitting in machine learning.â€
â€œWrite a Python function to calculate factorial.â€
â€œWhatâ€™s new in Llama 3 compared to GPT-4?â€

ğŸ”’ Notes
 Ollama must be running locally for the app to work.
 Make sure to process PDFs before asking questions.
 If no text is extracted, try uploading a text-based PDF (not a scanned image).

ğŸ’¡ Future Improvements
 Add OCR support for scanned PDFs using Tesseract
 Include conversation memory for contextual chat
 Support multiple document uploads and cross-document Q&A
 Add UI themes and chat history saving

ğŸ‘¨â€ğŸ’» Author
Ganesh Prasad Sahoo
ğŸ’¼ B.Tech CSE | Centurion University of Technology and Management
ğŸŒ LinkedIn : https://www.linkedin.com/in/ganesh-prasad-sahoo-775346293/
